# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CPUWibFnjCBKeQtDe8wG2dCc3n2OPcWy

# Implement Naive Bayes classifier algorithm for two class classification
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import random
import os

from sklearn.metrics import classification_report,mean_absolute_error,mean_squared_error,confusion_matrix,accuracy_score, roc_auc_score, f1_score, roc_curve
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import MinMaxScaler, StandardScaler


ds = pd.read_csv("/content/wine.csv")
ds.head()


ds['quality_cat'] = ds['quality'].astype('category').cat.codes
ds.head()

data1 = ds.drop('quality',axis=1)
data1.info()

Y = data1['quality_cat']
Y.head()

X = data1.drop('quality_cat',axis=1)
X.head()


from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y,test_size=0.2,random_state=45)



class NaiveBayesClassifier:
    def __init__(self):
        self.classes = None
        self.class_priors = None
        self.class_means = None
        self.class_stds = None

    def fit(self, X, y):
        self.classes = np.unique(y)
        n_classes = len(self.classes)
        n_features = X.shape[1]
        self.class_priors = np.zeros(n_classes)
        self.class_means = np.zeros((n_classes, n_features))
        self.class_stds = np.zeros((n_classes, n_features))

        for i, c in enumerate(self.classes):
            X_c = X[y == c]
            self.class_priors[i] = X_c.shape[0] / X.shape[0]
            self.class_means[i, :] = X_c.mean(axis=0)
            self.class_stds[i, :] = X_c.std(axis=0)

    def predict(self, X):
        posteriors = np.zeros((X.shape[0], len(self.classes)))

        for i, c in enumerate(self.classes):
            prior = np.log(self.class_priors[i])
            posterior = prior + np.sum(np.log(self._pdf(X, self.class_means[i, :], self.class_stds[i, :])), axis=1)
            posteriors[:, i] = posterior

        return self.classes[np.argmax(posteriors, axis=1)]

    def score(self, X, y):
        y_pred = self.predict(X)
        accuracy = np.mean(y_pred == y)
        return accuracy

    def _pdf(self, X, mean, std):
        eps = 1e-6
        variance = std ** 2
        return np.exp(-0.5*((X - mean)**2)/(variance + eps))/np.sqrt(2*np.pi*(variance + eps))

    def get_params(self, deep=True):
        return {}


nbc = class Naive Bayeslassifier()
nbc.fit(X_train, Y_train)

print(X_test.values[0])
print(Y_test[0])


Y_nbc_pred = nbc.predict(X_test)
print(X_test.values[0])
print(Y_nbc_pred[0])


NBCAccuracy = accuracy_score(Y_test, y_hat)
print(f"Naive Bayes Classifier Accuracy: {NBCAccuracy}")


from sklearn.metrics import f1_score
f1_score = f1_score(Y_test, y_hat)
print('Naive Bayes Classifier F1 score: %f' % f1_score)

"""# Implement Logistic regression"""

class LogisticRegression:
    def __init__(self, learning_rate=0.1, num_iterations=1000):
        self.learning_rate = learning_rate
        self.num_iterations = num_iterations
        self.weights = None
        self.bias = None
        
    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def fit(self, X, y):

        num_features = X.shape[1]
        self.weights = np.zeros(num_features)
        self.bias = 0
        

        for i in range(self.num_iterations):
            z = np.dot(X, self.weights) + self.bias
            y_pred = self.sigmoid(z)
            error = y_pred - y
            gradient = np.dot(X.T, error) / len(y)
            self.weights -= self.learning_rate * gradient
            self.bias -= self.learning_rate * np.mean(error)
    
    def predict(self, X):
        z = np.dot(X, self.weights) + self.bias
        y_pred = self.sigmoid(z)
        return np.round(y_pred).astype(int)
    
    def score(self, X, y):
        y_pred = self.predict(X)
        accuracy = np.mean(y_pred == y)
        return accuracy
    
    def get_params(self, deep=True):
        return {'learning_rate': self.learning_rate, 'num_iterations': self.num_iterations}


l_r = LogisticRegression()
l_r.fit(X_train ,Y_train)


Y_lr_pred = l_r.predict(X_test)

from sklearn.metrics import accuracy_score
from sklearn import metrics


LRAccuracy = metrics.accuracy_score(Y_test,Y_lr_pred)
print('Logistic Regression Accuracy: %f' % LRAccuracy)


from sklearn.metrics import f1_score
F1_score = f1_score(Y_test, Y_lr_pred)
print('Logistic Regression F1 score: %f' % F1_score)